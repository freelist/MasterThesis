\chapter{Typical Problems}\label{ch:problems}
In this Chapter the reader will became familiar with typical formulations of SLAM problems, with a particular focus on 3D environments. Obviously, there are several other instances of the problem that will be not shown since they are not strictly related to this work.

\section{Pose Graphs}\label{sec:pose_graphs}
Pose Graphs represents the backbone of SLAM. In this problem, the state vector $\state = \left(\state_1, ..., \state_N\right)$ is composed by a 2D or 3D isometry, thus, each node of the graph $\state_i$ belongs to $SE(2)$ or $SE(3)$ - i.e. the \textit{Special Euclidean} group of dimension $2$ or $3$.

The measurements are also 2D or 3D isometries that lie in $SE(2)$ or $SE(3)$. Therefore, an edge $\meas_{ij}$ that connects $\state_i$ with $\state_j$ represents the pose of node $j$ expressed in the reference frame of node $i$ - e.g. $\meas_{ij} =\: \T{i}{j}$. 

This problem is very common in SLAM: suppose that we want to estimate the best trajectory of a 2D robot, given only the \textit{odometry} measurements. The odometry retrieves robot's motion from state $\state_i$ to $\state_j$ and encodes it into the transformation matrix $\T{i}{j}$. Moreover, supposing that the robot is also able to retrieve \textit{loop-closures}, this would mean that there is an edge between the nodes $i$ and $k$. The related transformation is encoded into the quantity $\meas_{ik} =\: \T{i}{k}$.

Clearly, both states and measurements are non-euclidean. The extended parametrization of those quantities - as it has been stated before - is given by an isometry, while a possible \textit{minimal representation} can be a $3$ vector $(t_{x}\: t_{y}\: \theta)^T$. The next step is to define the operators \textit{box-plus} and \textit{box-minus}. Therefore, we introduce the operators $t2v$ and $v2t$ that allow to map an isometry into a $3$ vector and vice versa. Given those operators, we will have the following relations:

\begin{align}
    \label{eq:boxplus_se2}
    \SState \boxplus \dx &= \SState \cdot \v2t(\dx)\\
    \label{eq:boxminus_se2}
    \SState_a \boxminus \SState_b &= \text{t2v}\left(\SState_b^{-1} \, \SState_a\right)
\end{align}

Since the measurement $\meas_{ij}$ expresses pose $\SState_j$ with respect to the reference system of $\SState_i$, the predicted measurement $\meas_{ij}$ can be computed as 

\begin{equation}
    \label{eq:prediction_se2}
    \Pred_{ij} = h_{ij}(\SState) = \SState_i^{-1} \, \SState_j
\end{equation}

\noindent Given the relations \ref{eq:boxminus_se2} and \ref{eq:prediction_se2}, the error is computed as follows:

\begin{align}
    \error_{ij}\left(\SState\right) &= h_{ij}\left(\SState\right) \boxminus \Meas_{ij} = \nonumber \\
    &= \text{t2v}\left(\Meas_{ij}^{-1}\, \SState_i^{-1} \,\SState_j\right)
    \label{eq:error_se2}
\end{align}

Finally, Jacobians must be computed and, to do so, we apply a perturbation to the error function \ref{eq:error_se2}, that leads to the following relation:

\begin{align}
    \error_{ij}\left(\SState \boxplus \dx\right) &= \text{t2v}\left(\Meas_{ij}^{-1} \, \left(\SState_i\cdot\v2t(\dx_i)\right)^{-1}\: \left(\SState_j\cdot\v2t(\dx_j)\right) \right) = \nonumber\\
    &= \text{t2v}\left(\Meas_{ij}^{-1} \, \v2t(\dx_i)^{-1}\: \underbracket{\SState_i^{-1}\SState_j}_{\Pred_{ij}} \:\v2t(\dx_j) \right) = \nonumber \\
    &= \text{t2v}\left(\Meas_{ij}^{-1} \, \v2t(\dx_i)^{-1}\: \Pred_{ij} \:\v2t(\dx_j) \right)
    \label{eq:error_perturbation_se2}
\end{align}

\noindent The remaining part is just the computation of the following partial derivatives:

\begin{align}
    \label{eq:jac_i_se2}
    \jacob_i = \frac{\partial\: \text{t2v}\left(\Meas_{ij}^{-1} \, \v2t(\dx_i)^{-1}\: \Pred_{ij} \:\v2t(\dx_j) \right)}{\partial \dx_i} \Bigg\rvert_{\dx_i = 0, \dx_j=0} \\
    \label{eq:jac_j_se2}
    \jacob_j = \frac{\partial\: \text{t2v}\left(\Meas_{ij}^{-1} \, \v2t(\dx_i)^{-1}\: \Pred_{ij} \:\v2t(\dx_j) \right)}{\partial \dx_j} \Bigg\rvert_{\dx_i = 0, \dx_j=0}
\end{align}

\noindent The final Jacobian will be non-zero only in the blocks relative to variables $\SState_i$ and $\SState_j\,$, in formul\ae:

\begin{equation}
    \jacob = \left[\zero \: \cdots \: \zero \:\, \jacob_i \,\: \zero \: \cdots \: \zero \:\, \jacob_j \,\: \zero \: \cdots \: \zero\right]
    \label{eq:jac_se2}
\end{equation}

The reader might notice that the equation \ref{eq:error_perturbation_se2} is highly non-linear - as also the operators $\text{t2v}$ and $\v2t$ themselves - and this will lead to a less accurate approximation obtained from the first-order Taylor expansion of the error \ref{eq:error_se2} and to the computation of complex derivatives given by \ref{eq:jac_i_se2} and \ref{eq:jac_j_se2}. The situation becomes even worse in a 3D environment, which will be better analyzed in the next Chapters. 

\section{Pose-Landmark Graphs}\label{sec:pose_land_graph}
In this Section we will propose another common formulation of the problem. Landmarks represent the position of salient world points and here are employed to optimize the trajectory of the robot, assuming that the position of those points is known in the environment. 

Taking in consideration again the 2D problem for simplicity, the true position of those points in the world will be given by $\p = (p_x\: p_y)^T$. The measurements $\meas_k$ are the position of those point in the robot reference frame - namely $\meas_k = (z_x\:z_y)^T$. It is good to notice that while the state still belongs to $SE(2)$, the measurements now are Euclidean. Both the minimal and extended parametrization of the state can be taken from the previous formulation - so they will be a $3$ vector $\state = (t_x t_y \theta)$ and a 2D isometry $\SState =\: \T{W}{R}$ respectively. Also the operators $\boxplus$ and $\boxminus$ remain unchanged. The 2D isometry $\SState_i$ is composed by a rotational part $\rot_{\theta}$ and a translational one $\trans$, in formul\ae:

\begin{align}
    \SState = 
        \begin{bmatrix}
            \rot_{\theta} & \trans \\
            \zero & 1
        \end{bmatrix} 
    \qquad
    \rot_{\theta} = 
        \begin{bmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{bmatrix}
    \qquad
    \trans = 
        \begin{pmatrix}
            t_x \\
            t_y
        \end{pmatrix}
    \label{eq:2d_isometry}
\end{align}

Given those considerations, it is possible to define the new \textit{prediction} of the measurement:

\begin{align}
    \pred_{ij} = h_{ij}(\SState) &= \SState_i^{-1}\,\p_j \nonumber \\
    &= \rot_{\theta}^T\,\p_j - \rot_{\theta}^T\trans
    \label{eq:prediction_r2_temp}
\end{align}

Since the measurements are Euclidean - i.e. they lie in $\mathbb{R}^2$ - the error is just the standard subtraction between prediction and measurement, namely $\error_{ij}(\SState) = \pred_{ij} - \meas_j$. Applying a state perturbation to the error we obtain:

\begin{equation}
    \label{eq:error_perturbation_r2_temp}
    \error_{ij}(\SState \boxplus \dx) = \left(\SState_i\,\v2t(\dx_i)\right)^{-1}\p_j - \meas_j
\end{equation}

\noindent To facilitate the computation of the derivatives, it is possible to define the \textit{box-plus} operator as the pre-multiplication of $\v2t(\dx)$ to the state isometry, namely $\SState \boxplus \dx = \v2t(\dx)\,\SState$. Moreover, instead of considering the state as $\SState = \,\T{W}{R}$ - i.e. the transformation from world to robot expressed in the world reference frame - it is more advisable to use its inverse. Therefore, the state now is $\SState = \,\T{R}{W} = \,\T{W}{R}^{-1}$ - i.e. the transformation \textit{from robot to world} expressed in the robot reference frame. Introducing this notation together with the new \textit{box-plus} operator, equations \ref{eq:prediction_r2_temp} and \ref{eq:error_perturbation_r2_temp} become:

\begin{align}
    \label{eq:prediction_r2}
    \pred_{ij} = h_{ij}(\SState) &= \SState_i \, \p_j = \rot_{\theta} \p_j + \trans \\
    \label{eq:error_perturbation_r2}
    \error_{ij}(\SState \boxplus \dx) &= \v2t(\dx) \underbracket{\SState_i\, \p_j}_{\tp_j} - \meas_j
\end{align}

\noindent The Jacobian is now more straightforward and it is computed as follows:

\begin{align}
    \jacob_i &= \frac{\partial \error_{ij}(\SState \boxplus \dx)}{\partial \dx} \Bigg \rvert_{\dx = 0} = \nonumber \\
    &= \frac{\partial\begin{pmatrix}
            \tilde{p}_x\cos(\Delta\theta) - \tilde{p}_y\sin(\Delta\theta) + \Delta t_x \\
            \tilde{p}_x\sin(\Delta\theta) + \tilde{p}_y\cos(\Delta\theta) + \Delta t_y
        \end{pmatrix}}{\partial\dx} \Bigg \rvert_{\dx = 0} = \nonumber \\
    &= \begin{bmatrix}
            I_{2 \times 2} & \begin{matrix} -\tilde{p}_y \\ \tilde{p}_x\end{matrix}
    \end{bmatrix}
    \label{eq:jacobian_r2}
\end{align}

Now that we defined every aspect of the problem, it is possible to embed everything in the already seen Least-Squares algorithm to find the optimal state.

\section{Pose and Landmark Estimation}\label{sec:landmark_full_SLAM}
This problem instantiation is more complex with respect what has been proposed in the previous Sections. In Section \ref{sec:pose_graphs} the constraints were only of type \textit{pose} - i.e. they lie on $SE(n)$ - while in Section \ref{sec:pose_land_graph} they were only of type \textit{point} - i.e. they lie on $\mathbb{R}^n$.

In this case we want to estimate \textit{both} the robot trajectory \textit{and} the world position of the landmarks, given constraints of type \textit{pose} and \textit{point}. Considering again the 2D case for simplicity, the state will be:

\begin{equation}
    \SState = \left(\overbracket{\SState^R_1, ..., \SState^R_N}^{N \;poses}\: |\: \overbracket{\state^L_{N+1}, ..., \state^L_{N+M}}^{M \;landmarks}\right)
    \label{eq:state_ba_2d}
\end{equation}

\noindent where $\SState^R_i = \,\T{W}{R} \in SE(2)$ represents the $i$-th robot pose while $\state^L_j = \p = \left(p_x p_y\right)^T \in \mathbb{R}^2$ represents the world position of the $j$-th landmark. The set of increments $\dx$ will have the structure seen in \ref{eq:state_ba_2d}, namely:

\begin{equation}
    \dx = \left(\overbracket{\dx^R_1, ..., \dx^R_N}^{N \;poses}\: |\: \overbracket{\dx^L_{N+1}, ..., \dx^L_{N+M}}^{M \;landmarks}\right)
    \label{eq:increments_ba_2d}
\end{equation}

\noindent where $\dx^R_i = (\Delta t_{x_i}\, \Delta t_{y_i}\, \Delta \theta_i)^T$ and $\dx^L_j = (\Delta p_{x_j}\, \Delta p_{y_j})^T$.

In order to properly apply the increment, it is necessary to define a suitable operator \textit{box-plus}. In this case, for the \textit{point} increments it is possible to use the Euclidean sum, while for the \textit{pose} ones we will employ the already seen operator $\boxplus$, leading to $\SState \boxplus \dx^R = \v2t(\dx^R)\, \SState$.

Clearly, depending on the types of constraint that we linearize, it leads to different contribution in the \textit{Hessian} matrix $\hessian$, so they will be analyzed separately.

\subsection{Pose-Pose Constraints}\label{subsec:pose_pose_constraints_2d}
Here, for a 2D environment, it is possible to reuse what it has been shown in Section \ref{sec:pose_graphs}. Therefore, the \textit{predicted measurement} $\Pred_{ij}$, the \textit{error} $\error_{ij}$ and the \textit{perturbed error} are computed as follows:

\begin{align*}
    \Pred_{ij} &= h_{ij}(\SState) = \SState_i^{-1}\, \SState_j \\
    \error_{ij}(\SState) &= \text{t2v}\left(\Meas_{ij}^{-1}\,\SState_i^{-1}\, \SState_j\right) \\
    \error_{ij}(\SState \boxplus \dx^R) &= \text{t2v}\left(\Meas_{ij}^{-1} \, \v2t(\dx^R_i)^{-1}\: \Pred_{ij} \:\v2t(\dx^R_j) \right)
\end{align*}

\noindent The complete Jacobian will be structured as in equation \ref{eq:jac_se2} and its components can be computed employing the \textit{chain-rule} for partial derivatives.

\subsection{Pose-Point Constraints}\label{subsec:pose_point_constraints_2d}
It is possible to refer to what it has been shown in Section \ref{sec:pose_land_graph} for this part. The reader might notice that in this formulation the \textit{robot pose} is expressed in the world reference frame - i.e. $\SState =\, \T{W}{R}$ - and, thus, we must stick to this notation also for \textit{pose-point constraints}. Therefore, supposing that the measurement $\meas_{ij}$ relates the $i$-th pose with the $j$-th point - indicated with $\SState_i$ and $\state_j = \p_j$ respectively - the \textit{predicted measurement} $\pred_{ij}$, the \textit{error} $\error_{ij}$ and the \textit{perturbed error} are computed as follows:

\begin{align*}
    \pred_{ij} = h_{ij}(\SState) &= \SState_i^{-1} \, \p_j = \rot_{i}^T\,\p_j - \rot_{i}^T\trans_i \\
    \error_{ij}(\SState) &= \pred_{ij} - \meas_j \\
    \error_{ij}(\SState_i \boxplus \dx_i^R, \state_j + \dx_j^L) &= \left(\v2t(\dx_i^R)\SState_i\right)^{-1}\left(\p_j + \dx_j^L\right) - \meas_j
\end{align*}

In this formulation the state parameters are $\SState_i$ \textit{and} $\p_j$, the final Jacobian $\jacob$ will have the following structure:

\begin{equation*}
    \jacob = \left[\zero \: \cdots \: \zero \:\, \jacob_R \,\: \zero \: \cdots \: \zero \:\, \jacob_L \,\: \zero \: \cdot \: \zero\right]
\end{equation*}

\noindent where 

\begin{align}
    \label{eq:jac_p_se2}
    \jacob_R &= \frac{\partial \left[\left(\v2t(\dx_i)\SState_i\right)^{-1}\left(\p_j + \dx_j\right) - \meas_j\right]}{\partial\dx_i^R}\Bigg \rvert_{\dx_i^R = 0, \dx_j^L = 0} \\
    \label{eq:jac_l_se2}
    \jacob_L &= \frac{\partial \left[\left(\v2t(\dx_i)\SState_i\right)^{-1}\left(\p_j + \dx_j\right) - \meas_j\right]}{\partial\dx_j^L}\Bigg \rvert_{\dx_i^R = 0, \dx_j^L = 0}
\end{align}

\noindent The reader might notice how this formulation leads to more complex derivatives with respect to the one reported in equation \ref{eq:jacobian_r2}.

\vspace{10px}

Once that all the needed objects are defined, it is possible to embed them in the iterative algorithm to find the optimal state configuration.

\section{Bundle Adjustment}\label{sec:bundle_asjustment}
The Bundle Adjustment problem is a very common formulation in \textit{Photogrammetry} and it is similar to the pose-landmark one seen in Section \ref{sec:pose_land_graph}. In this case, the state is composed by both \textit{camera poses} - $SE(3)$ objects - and 3D points, but those last ones are observed through their \textit{projection} onto the \textit{image plane}.

Therefore, given a set of 3D points $\SState_j$ viewed by a set of camera matrices $P^i$ and indicating with $\state_j^i$ the image coordinates of the $j$-th point projected through the $i$-th camera, we want to find the most-likely configuration of camera matrices and 3D points that suits the observations. This formulation corresponds to the minimization of the following cost function:

\begin{equation}
    \min_{\hat{P}^i, \hat{\SState}_j} \sum_{i,j} d\left(\hat{P}^i\,\hat{\SState}_j, \state_j^i\right)^2
    \label{eq:reprojection_error}
\end{equation}

\noindent where $d(\cdot, \cdot)$ is the Euclidean distance function. The distance between the 3D point projected through $P$ onto the image plane and the measured image coordinates is known as the \textit{reprojection error}.

The reader might have already noticed that the measurements lie on $\mathbb{R}^2$ - since they represent image points. Clearly, the prediction, error perturbation and Jacobian are also different, although those are not explicitly computed here, since they are not strictly related to this work; further information on this topic can be found in \cite{hartley2003multiple, szeliski2010computer, indelman2015incremental}.

\section{Simultaneous Calibration Localization and Mapping}\label{sec:sclam}
The formulations seen in the previous Sections, they all rely on the fact that all the specific \textit{inner robot parameters are known} - e.g. the position of the sensors mounted on the robot with respect to the robot reference frame. Performing SLAM with a wrong estimation of those parameters reduces the accuracy of the system, leading to non-consistent estimations even after the optimization process.

In general to acquire those information, it is possible to proceed in different ways:

\begin{itemize}
    \item Get those parameters from the robot's specifics - if they are present;
    \item Measure those quantity \textit{manually} on the robot;
    \item Perform ad-hoc calibration \textit{before} employing the robot in a mission.
\end{itemize}

\noindent All those procedures share the same drawbacks: they are not able to estimate non-stationary parameters and, if the robot is subject to hardware changes, the calibration procedure must be repeated. 

A possible solution to this problem is to \textit{embed those parameters in the state} and to estimate  them together with the robot trajectory and the map. This solution has been proposed by \textit{K\"ummerle et al.} in their work \cite{kummerle2011sclam} and it leads to several benefit to the original SLAM problem. Therefore, in this Section it is proposed a brief overview of this approach.

Given a generic robot in a 2D environment, we have to introduce the following quantities:

\begin{itemize}
    \item $\l = (l_x \, l_y \, \theta_l)$ the pose of a generic sensor expressed with respect to the robot reference frame;
    \item $\u_i$ and $\Omega_i^u$ that represent respectively the \textit{motion command} and its relative information matrix;
    \item $\k$ the parameters of the robot's \textit{forward kinematic}.
\end{itemize}

The forward kinematic function $K(\u, \k)$ converts the wheels' velocities deriving from the motion commands into the actual robot displacement from node $i$ to $i+1$. For example, taking in consideration a differential drive kinematic scheme, the motion commands might be $\u = (v_r \, v_l)^T$ - respectively the left and right wheel velocity. Those lead to the following relation to compute the relative motion obtained during a time step $\Delta t$:

\begin{equation}
    \label{eq:dd_kinematics}
    K(\u, \k) = 
        \begin{pmatrix}
            \rot(\Delta t \, \omega) & \zero \\
            \zero & 1
        \end{pmatrix}
        \begin{pmatrix}
            -ICC \\ 0
        \end{pmatrix}
        +
        \begin{pmatrix}
            ICC \\ \Delta t \, \omega
        \end{pmatrix}
\end{equation}

\noindent where 

\begin{align*}
    ICC &= 
        \begin{pmatrix}
            0 \\ \frac{b}{2} \frac{r_l v_l + r_r v_r}{r_l v_l - r_r v_r}
        \end{pmatrix} \\
    \omega &= \frac{r_l v_l - r_r v_r}{b}
\end{align*}

\noindent In this robot configuration $r_l$ and $r_r$ represent respectively the left and right wheel radii, while $b$ is the distance between the two driving wheels. Those quantities represent the forward kinematic parameters $\k = (r_r\,r_k\,b)^T$ that we embed in the estimation process.

Given those objects, the optimization process has to retrieve the optimal configuration $\left[\state^\star \, \l^\star \, \k^\star\right]$ that minimizes the following negative log-likelihood function $F(\state, \l, \k)$:

\begin{equation}
    F(\state, \l, \k) = 
        \sum_{i,j} \error_{ij}^l(\state)^T\Omega_{ij}^l\,\error_{ij}^l(\state) + 
        \sum_{i} \error_{i}^u(\state)^T\tilde{\Omega}_{i}^l\,\error_{i}^u(\state)
\end{equation}

\noindent where $\error_{ij}^l(\state)$ describes how well the parameter blocks $\state_i$, $\state_j$ and $\l$ satisfy the constraint $\meas_{ij}$, while $\error_{i}^u(\state)$ measures the likelihood of the parameter blocks $\state_i$, $\state_j$ and $\k$ with respect to the constraint $\u_i$. The two error functions $\error_{ij}^l(\state)$ and $\error_{i}^u(\state)$ are described analytically by the following relations:

\begin{align*}
    \error_{ij}^l(\state) &= \left(\left(\state_j \boxplus \l\right) \boxminus \left(\state_i \boxplus \l\right)\right) \boxminus \meas_{ij} \\
    \error_{ij}^l(\state) &= \left(\state_{i+1} \boxminus \state_i\right) \boxminus K(\u_i, \k)
\end{align*}

\noindent where the operators $\boxplus$ and $\boxminus$ are the same of the ones described in the previous Sections. It is good to notice that, in order to obtain consistent results, the information matrix $\Omega_{i}^u$ must be projected through the forward kinematic function $K(\u, \k)$, for example using the \textit{Unscented Transform} \cite{julier2002scaledUT}.

\vspace{15px}

Given the main notions about the most common formulations of the problem, in the next Chapter it will be analyzed more in detail the solution for 3D factor graphs, explaining also the main contributions brought by this work.