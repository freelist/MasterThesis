\chapter{Introduction}\label{ch:intro}
Computer vision is one of the most prominent and prolific area of the computer science research, in particular perception and image processing are gaining more and more interests among the community. This work focuses the attention on machine and robotic perception in industrial settings. In those scenarios, having accurate algorithms and procedures is extremely important, since results may influence the overall productive chain. In this sense, many industrial realities, as like as big \emph{ICT (Information Communication Technology)} companies, are investing a lot in improving production systems, most of the time introducing new technologies and tools within the community, but also promoting events and challenges all over the world for enforcing researchers in pushing always out of state-of-the-art limits. That is the case of international competitions like \emph{Amazon Picking Challenge}\footnote{https://www.amazonrobotics.com/} and the \emph{RoboCup@Work}\footnote{http://www.robocupatwork.org/}, which is also the one in which we are actively involved as participating team and part of the Organizing Committee (OC) (the author of this thesis is part of the Robocup@Work OC from 2016). Given this experience we tried to extrapolate needs and requirements of the recent research in industrial settings and then propose our solution to the problem of testing and rigorously assert novel ideas, algorithms, software and tools in general.

\section{Motivations and Contributions}\label{sec:motivations}
Having rigorous test bed as benchmark for novel technologies, theories and methods is one of the key factor for improving more and more the research in every field. This concept is even more prominent in highly structured and rigorous settings such as the industrial one. We focused in particular on those scenarios and tried to propose rigorous benchmark procedures and tools for testing widely spread sensors, such as \emph{Microsoft Kinect 2} or \emph{Intel Realsense SR300} involving standard tools of common use in research in industrial settings, e.g. \emph{RoboCup@Work} objects. As like as common commercial sensors, over the years, industry has seen a very fast growth of custom and complex sensors, such as structured light sensors, active and passive stereo perceptive systems and many others. For those last mentioned sensor categories, actually no publicly available datasets existed up to the 2017 when MVTecc released its ITODD dataset \cite{mvtec2017itodd}, for this reason with this work we also introduce a novel dataset, the \emph{RAW Dataset}, built with multiple sensors with different features, namely \emph{passive} and \emph{active stereo cameras} and \emph{structured light sensors}. The dataset provides both rgb and depth images from 15 different scenes of objects in multiple arrangement, there are scenes with no clutter at all and other completely randomly built. All the scenes are provided with the ground truth 6D position of the objects. The ground truth will be used for evaluating some state-of-the art techniques for object detection and localization, as like as 3D scene reconstruction algorithms. This investigation will focus on properties that are important for practical applications in robotic industry, such as comparing the full 3D rigid transformation of the objects, evaluating the goodness of objects' bounding box detection, and some other relevant and specific 3D reconstruction approaches. The sensor setup of this dataset also introduces some novelty, namely we tested and developed a new sensor for passive and active stereo vision, the \emph{FlexSight Sensor}, built in fulfillment, behalf and supported by the European Community's project ECHORD++\footnote{http://echord.eu/}.

\section{Related Works}\label{sec:relatedwork}
Many recent and past works need to be mentioned if compared to our work, especially when talking about benchmarking tools like datasets or similar test beds. Several datasets for 3D object detection were introduced in the past, in particular, the work of Fireman \cite{Firman2016DatasetsReview} gives a detailed and complete review of what is the past and present research line about that. Particularly interest on 3D object pose datasets has been given in the work of Hodan et al. \cite{hodan20166DPoseEstimation}, where a comprehensive list of dataset and their tools and metrics is shown in detail.

From the introduction into the market of commercial sensors such as the aforementioned Kinect 2 or Realsense SR300, many datasets have been acquired and made publicly available, but the most of them are of less interest for industrial application. On the contrary, the work from Hodan et al. \cite{hodan2017tless}, introduced theso called \emph{T-LESS Dataset}, a large and very interesting dataset of texture-less objects oriented to the industrial settings. This dataset will be further presented in Sec. \ref{subsec:tless_dataset}. On the same line of the T-LESS Dataset, also the work from MVTec \cite{mvtec2017itodd} has recently introduced interesting tools for evaluating algorithms and techniques of object detection and 3D pose estimation in complex and concrete industrial settings, involving again texture-less objects with high relevance for manufacture and industrial scenarios. 

Our proposed RAW Dataset has similar focus as the MVTec ITODD and T-LESS datasets, similar in evaluation and sensor setups, while introducing different approaches in the acquisition phase since our sensor setup is not fixed as the other two and is much more inspired to real industrial applications where cameras, sensors in general, are mounted directly at the end-effector of a robotic manipulator, so they move while it moves.

\section{Structure of the Thesis}\label{sec:thesisstructure}
In the following chapters an overview of the state-of-the-art about perception in industry will be given (Chapter \ref{ch:perceptionandsensing}), focusing the attention on sensors technology (\secref{sec:sensor_techs}), 3D Scene Reconstruction and Object Detection techniques (Sec. \ref{sec:3dreconstruction} and \ref{sec:objectdetection}), and state-of-the-art software libraries commonly used in industrial settings (Sec. \ref{sec:industrylibraries}). In Chapter \ref{ch:benchmarks_and_metrics} an overview of commonly used metrics (Sec. \ref{sec:metrics}) and benchmark datasets (Sec. \ref{sec:datasets}) are going to be presented. After this detailed view of techniques, theories and tools about perception in industry, in Chapter \ref{ch:raw_dataset} the RAW Dataset is presented in detail, with a large overview of its details and composition (Sec. \ref{sec:raw_features} and Sec. \ref{sec:raw_scenes_details}), the acquisition setup and procedure (Sec. \ref{sec:raw_setup_and_sensors} and \ref{sec:raw_acquisition_procedure}) and finally how ground truth has been computed (Sec. \ref{sec:ground_truth_estim}) giving also a large view of the developed tools (Sec. \ref{subsec:raw_labeltool} and \ref{subsec:pose_propagation}). In Chapter \ref{ch:experiments} some experiments on the aforementioned RAW Dataset are going to be presented and brief discussion will be derived (Sec. \ref{sec:exp_discussion}). Finally, in Chapter \ref{ch:conclusions} final conclusions will be given, and on going and future works will be announced. This document contains also an appendix (Appendix \ref{apx:appendix}) where some more details about the FlexSight project (\ref{apx:flexsight}) and the RoboCup@Work (\ref{apx:robocupatwork}) are given.