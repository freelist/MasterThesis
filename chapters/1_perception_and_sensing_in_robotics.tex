\chapter{Perception in Robotics: Applications, Sensors and Algorithms}\label{ch:perceptionandsensing}
In this chapter we will discuss about robot perception in industrial settings, focusing the attention on the most common applications and problems, such as Random Bin Picking (RBP) and Pick\&Place (PPT), the exploited sensors, and the algorithms and techniques used to solve the presented problems. We will also introduce some state of the art commercial libraries that nowadays are commonly used in several industrial applications.
\section{Robotics in Industry}\label{sec:roboticsinindustry}
Over the years, industry has become one of the most important scenario where robots and automated systems have gained a widespread diffusion. This is basically thanks to the fact that robots can easily and quickly perform repetitive and complex tasks, while providing a constant and very high accuracy.
% : speed and accuracy are actually two central points of industrial productive chains.
% Manufacturing is one of the industrial sectors that saw the robotics ``era'' growing faster than elsewhere. 
Into the factories, robots are involved in every ring of the productive chain, from heavy loads handling to precise and accurate placing, from iron and metal soldering to small part assembly. The higher accuracy and speed that robots can reach, with so high levels of repeatability and precision, brought robots, manipulators in particular, at the top level of the industrial requirements.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/1_perception_and_sensing_in_robotics/amazon_picking_ch}
    \caption{\textbf{Amazon Picking Challenge example scenario.} In the images, the team is trying to detect and grasp objects from the red bin on the bottom, and place them on the shelf.} 
    \label{fig:amazon_picking_ch}
\end{figure}

The importance of robots in the industrial world is supported also by hi-tech giants, such as Amazon, Google and others, that in the last years demonstrated high interest in investing and developing their technologies in order to improve their performance and business. One interesting example is represented by the Amazon Picking Challenge competition proposed the first time at the International Conference on Intelligent Robots and Systems (IROS) in 2015 by the Amazon Robotics division\footnote{https://www.amazonrobotics.com}. This competition is thought to enhance and improve the research in the robotic manipulation, focusing the attention in grasping from random and highly cluttered environments, such as warehouse shelves or bins full of irregular and disorganized objects. In Figure \ref{fig:amazon_picking_ch}, some examples of typical tasks that teams involved in the Amazon Picking Challenge should compete.

\subsection{Random Bin Picking (RBP)}\label{subsec:binpicking}
Monotonous tasks such as unloading a bin one part at a time into a machine, bulk parts sorting, and order fulfillment are labor-intensive, moreover they can even be dangerous if the parts or operations are heavy, sharp and they change position every time one object is moved away. For years, bin picking robots have been tackling these tedious jobs, but there are still so many applications to be realized.

While more capable than ever, robotic bin picking still has its limitations. It is all matter of accuracy. While robots are applauded for their repeatability, random bin picking requires accuracy in the face of chaos. The robot has to locate a part in free space, in an unstructured environment where the parts keep shifting positions and orientations every time a part is removed from the bin. That requires a delicate balance between robotic dexterity, machine vision, software, computing power to crunch all the data in real time, and a grasping solution to extract the parts from the bin. All those highly technical challenges give to RBP higher complexity and difficulty.

First of all, a brief introduction to all the kinds of Bin Picking must be given. There are three main types of bin picking: structured, semi-structured, and random bin picking. Each presents an increasing level of application complexity, cost and cycle time:

\begin{itemize}
	\item \textbf{Structured}. Parts are positioned or stacked in the bin in an organized, predictable pattern, so they can be easily imaged and picked.
	\item \textbf{Semi-Structured}. Parts are positioned in the bin with some organization and predictability to help aid imaging and picking.
	\item \textbf{Random}. Parts are in totally random positions in a bin, including different orientations, overlapping, and even entangled, further complicating the imaging and picking functions.
\end{itemize}

From now on we will concentrate on Random Bin Picking, that is the main area in which we tested our work, moreover, the RAW dataset that will be presented later in this thesis and all the developed tools, have been thought for such robotic task in particular.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/1_perception_and_sensing_in_robotics/rbp_example}
    \caption{\textbf{Random Bin Picking Example.} Robotic random bin picking and part loading system uses 3D vision guided robots with magnetic grippers to locate and pick parts for a heat treating operation.} 
    \label{fig:rbp_example}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/1_perception_and_sensing_in_robotics/cluttered_env_rbp}
    \caption{\textbf{Machine vision algorithm execution example in RBP task.} Here, a 3D area sensor is performing localization and recognition of random parts in a bin. Thanks to this technology, the robot should be capable of planning its next pick in the clutter.} 
    \label{fig:cluttered_env_rbp}
\end{figure}

An example of a robotic system performing random bin picking operation is depicted in Figure \ref{fig:rbp_example}. In this kind of scenarios, the role of vision is crucial. Vision systems must be capable of recognize, detect and localize the parts with extremely high accuracy. Typical scenarios are like the one in Figure \ref{fig:cluttered_env_rbp}, it's clearly visible how the environment is cluttered, all the parts are randomly spread inside the bin, and the task of recognizing and localizing the next part to be picked, it's extremely difficult. 

\subsection{Pick\&Place Tasks (PPT)}\label{subsec:pickandplace}
More generic, but with the same importance as the previous one, Pick\&Place (PPT) task is another interesting robotic application in which vision and perception play an important role. Differently from the aforementioned Bin Picking applications, in Pick\&Place scenarios, camera setup is not always fixed, they are mounted on the robotic arm, and the arm itself is not said to be fixed on the ground. 

A clear example of Pick\&Place scenario is the RoboCup@Work Competition, where the robotic arm that performs pick and drop operations with the objects, is actually mounted on a mobile platform that operates in a more large environment. Another example of robotic Pick\&Place operation are the robot assisted assembly \cite{evangelista2017grounding}, in which the robot assists the human in performing operation in industrial scenarios guided by natural language interaction.

\section{Sensors Technologies}\label{sec:sensor_techs}
In this section, we provide some details about the sensor technologies and a selection of relevant and state-of-the-art sensors typically employed in the pick\&place and bin-picking applications (\secref{sec:roboticsinindustry}). The presented use cases represent only a small portion of the 3D sensor available in the market: anyway, almost all other sensors exploit one of the technologies described below.\\
Most of state-of-the-art industrial systems currently works with 3D sensors, sometimes coupled with a color or a gray-level camera, since they enable to obtain in a direct way the 3D position of the objects of interest. While almost all RGB cameras are currently based on standard and mature technologies (i.e., they are based on CCD or more frequently CMOS sensors), depth information can be obtained using several types of technology, each one with its strengths and weaknesses.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/active_vs_passive}
    \caption{\textbf{Passive Stereo sensor VS Active Stereo sensor}. Passive stereo camera (left) VS. active depth sensor (right) with colored pattern projector (image taken from \url{http://eia.udg.es/~qsalvi/Tutorial_Coded_Light_Projection_Techniques_archivos/frame.html}​).} 
    \label{fig:active_vs_passive}
\end{figure}

\subsection{Passive VS Active Sensor}\label{subsec:passiveVsActive}
Traditionally depth information has been acquired using passive stereo cameras, i.e. cameras with two or more separate image sensors. Depth of 3D points are recovered by means of a correspondence problem: matched points projections are triangulated between pairs of sensors. 
Just like monocular cameras, stereo cameras belong to the class of passive sensors, i.e. sensors that do not modify the surrounding environment in order to acquire data.
Unfortunately these systems fail in absence of salient visual features in the surfaces of the framed scene: for this reason, only very few modern systems are currently based on this type of technology.

On the other side, active sensor uses light emitters that project a specific pattern or a light with a specific wavelength \emph{(​Time-of-Flight sensors, ToF)}: all these sensors modify in some way the surrounding environment (i.e., they illuminate the scene). The correspondence problem in this case is solved, e.g., by searching the known pattern in the camera image (pattern decoding, used in ​ \emph{Structured Light sensors, SL}) or performing a traditional stereo matching algorithm using visual features synthetically created by the light projector (​\emph{Active Stereo sensors, AS}).
In the first case (SL) just one camera is required, but the camera-projector couple should be carefully calibrated. In the second case two calibrated cameras are required, while an accurate calibration between the cameras and the projector is not required: the projector is just required to illuminate the framed scene.

An qualitative comparison of Active Stereo sensor and Passive Stereo sensor is given in \figref{fig:active_vs_passive}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/tof_sensors}
    \caption{\textbf{Time-of-light sensor technology}. LIDAR range scanner (left, images taken form ​\url{https://it.wikipedia.org/wiki/Lidar​}) and time-of-flight array principle (right image taken from \cite{Shim2012tof}).} 
    \label{fig:tof_sensors}
\end{figure}

\subsection{Time-of-Flight Sensors}\label{subsec:tof_sensors}
ToF sensors works by emitting a typically IR light carrier with known wavelength, then measuring the phase shift of the reflected light on the receiver side (Fig.2 right). 2D LIDAR (Laser Imaging Detection and Ranging) range scanners (Fig.2 left) use one panning emitter-receiver couple, while ToF cameras (Fig.2 right) use several emitters and a matrix of receivers that provide a depth map of the framed scene.

\begin{itemize}
	\item \textbf{Pros}: ToF cameras are compact sensors that can provide full depth maps in real time, well suited also for moving devices.
	\item \textbf{Cons}: The depth map resolution is generally low, while the accuracy is not always adequate, in particular close to surfaces' edges and corners. Generally they are not very robust when dealing with reflective materials or black surfaces.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/1_perception_and_sensing_in_robotics/basler_tof_camera}
    \caption{\textbf{The Basler ToF Camera}.} 
    \label{fig:basler_tof_camera}
\end{figure}

A novel sensor in the ToF area is the \emph{Basler ToF Camera}, depicted in \figref{fig:basler_tof_camera} This new sensor provides cutting edge features in the field of the ToF cameras: it has a relatively high resolution (640 X 480) with high frame rate (20 fps) and a large working range. Unfortunately, like other ToF sensors, the accuracy (+/- 1cm) is often not adequate for many industrial applications.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/1_perception_and_sensing_in_robotics/sl_sensors}
    \caption{\textbf{Structured light camera basic ​ principle}. This image is taken from \url{​ http://blog.teledynedalsa.com/​}.} 
    \label{fig:sl_sensors}
\end{figure}

\subsection{Structured Light Sensors with Moving Head}\label{subsec:sl_sensors_mh}
Structured light sensors are based on light triangulation: they typically use a laser projector that thanks to a specific lens produces a known ``sparse'' pattern (i.e., typically a line). A conventional camera mounted at a known distance from the laser (​ baseline) frames the scene in order to obtain the distance from the illuminated 3D points. The most common structured light sensors use a laser line projector: such type of sensors can measure a profile for each image (see \figref{fig:sl_sensors}). It is possible to obtain a full 3D point cloud of the working area by integrating each measurements while moving the camera-laser couple over the whole area.

\begin{itemize}
	\item \textbf{Pros}: These type of sensors generally provide high quality 3D reconstructions while being quite robust when dealing with reflective materials.
	\item \textbf{Cons}: They usually require expensive devices to move the scanning head in order to be able to scan the entire area; the cycle time is increased by the scanning time (typically from than 1 to 3 seconds).
\end{itemize}

An example of this kind of sensor is the \emph{Sick IVC-3D} depicted in \figref{fig:sic_sensor} (left). Sick ​\footnote{https://www.sick.com} is one of the best known companies in the field of LIDARS and industrial structured light sensors. The Sick IVC-3D is a family of scanning heads able to triangulate a laser line in real-time, using an ​ integrated laser projector. It requires to be mounted on a mobile system since the laser beam is fixed: the sensor should be moved over a 2D plane. The sensor streams data over an ethernet interface, and it is factory calibrated.

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/1_perception_and_sensing_in_robotics/sic_sensor}
    \caption{\textbf{Some Structured Light sensors from Sick$^2$}. On the left the Sick ICV-3D, on the right the Sick Scanning Ruler S1200.} 
    \label{fig:sic_sensor}
\end{figure}

\subsection{Structured Light Sensors with moving pattern}\label{subsec:sl_sensors_mp}
A variant of the previous sensor can be obtained motorizing the laser line projector: in this case it is not necessary to move the scanning device, but this new solution usually brings to higher costs in production and sell.

\begin{itemize}
	\item \textbf{Pros}: These type of sensors generally provide high quality 3D reconstructions while being quite robust when dealing with reflective materials.
	\item \textbf{Cons}: The cycle time is increased by the scanning time (typically from 1 to 3 seconds).
\end{itemize}

An example of this sensor is the \emph{Sick Scanning Ruler S1200} reported in \figref{fig:sic_sensor} (right). It is a high-performance, factory calibrated, depth sensor which integrates a motorized laser projector. The field of view allows to detect objects inside an Euro pallet or an USA pallets, with a relatively high resolution ($756 \times 512$) and a high depth accuracy, typically from 1 mm to 4 mm, depending on the distance from the working area.

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/1_perception_and_sensing_in_robotics/sl_sensor_kinect1}
    \caption{\textbf{Structured Light Sensors with Dense Pattern example}. The Microsoft Kinect RGB-D camera (left) and the dot matrix pattern projected by its near-infrared illuminator (right, image extracted from the video ​\url{https://www.youtube.com/watch?v=nvvQJxgykcU&feature​}).} 
    \label{fig:sl_sensor_kinect1}
\end{figure}

\subsection{Structured Light Sensors with Dense Pattern}\label{subsec:sl_sensors_dp}
In order to avoid using mechanical devices to move the scanning head or the projector, some devices employ a dense pattern generator that illuminate the scene with a divergent dense pattern, commonly a $N \times M$ pseudo random dot matrix pattern (see \figref{fig:sl_sensor_kinect1}). The pattern is known and each pattern patch (a $P \times P$ submatrix, $P << N, M$) represents a unique binary matrix. Detecting the pattern patches from the camera and knowing the baseline between the camera and the pattern projector, it is possible to estimate the depths by means of triangulation. The classical example of such sensors is represented by the Microsoft Kinect RGB-D camera (Fig. 7 left): it uses a near-infrared projector and it provides a dense $320 \times 240$ depth map.

\begin{itemize}
	\item \textbf{Pros}: Generally compact sensors that can provide full depth maps in real time well suited also for moving devices.
	\item \textbf{Cons}: Hard to calibrate, not very robust against sunlight and other external light sources when using an eye safe projector.
\end{itemize}

An example of industrial SL with Dense Pattern laser sensor is the \emph{Pick-It 3D Camera} (See \figref{fig:pick_it_3d}). Pick-IT\footnote{​ www.pickit3d.com} introduced a complete bin-picking system for small parts that includes a custom-built depth sensor  that is, under a technological point of view, very similar to the Microsoft Kinect 1 RGB-D camera. Pick-It produces two versions of its sensor: one short-range (400 - 800 mm) and one long-range (600 - 1300 mm). None of these sensors can deal with container larger than $600 \times 800$ mm.

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/1_perception_and_sensing_in_robotics/pick_it_3d}
    \caption{\textbf{Pick-it 3D camera}. The Pick-it 3D camera (left) installed on top of a small box (right).} 
    \label{fig:pick_it_3d}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/1_perception_and_sensing_in_robotics/time_multiplexing_coding}
    \caption{\textbf{Pick-it 3D cameraTime-Multiplexing coding}.} 
    \label{fig:time_multiplexing_coding}
\end{figure}

\subsection{Active Stereo with Changing Pattern}\label{subsec:ac_stereo_changing_pattern}
As the conventional stereo pairs, active stereo sensors create depth maps by triangulating corresponding points projections. In AS, the projector illuminates the scene in order to simplify the points matching. A simple but effective way to match points between the two cameras is to project a sequence of light patterns so that every pixel is encoded with a \emph{codeword} identified by the pattern sequence. This technique is called ​ time-multiplexing coding, and the most common structure of the patterns is a sequence of stripes that decrease their thickness over time (single axis encoding, \figref{fig:time_multiplexing_coding}). These sensor are typically based on \emph{DLP (Digital Light Processing)} projectors.

\begin{itemize}
	\item \textbf{Pros}: For specific configurations and by using high-end projectors, these type of sensors can provide the most accurate 3D reconstructions, for this reason they are usually used for 3D printing applications.
	\item \textbf{Cons}: The cycle time is increased by the projection time (typically from 1 to 2 seconds). Due to the projector type (DLP), they are not robust against sunlight and other external light sources: usually these sensors require a protected environment with no external light sources.
\end{itemize}

As an example of this kind of technology, the sensor that needs to be mentioned here is the \emph{EnShape Inspect} sensor (See \figref{fig:enshape}) from EnShape\footnote{​ http://www.enshape.de/}. It is actually a Active Stereo sensor with changing \emph{pseudo-random} laser pattern. Dense pseudo random pattern projectors can be used also to perform active stereo scene reconstruction: since each pattern patch is unique, it is relatively easy to match their image projections between the two cameras. This sensor is also capable of providing 3D reconstruction in real-time, so it is well suited for moving devices, e.g. robotic manipulators, but it has only one limitation, it is extremely sensible to illumination differences since it uses eye safe laser projector so ambient light can easily influence the pseudo-random projected pattern.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/1_perception_and_sensing_in_robotics/enshape}
    \caption{\textbf{EnShape Inspect sensors family}.} 
    \label{fig:enshape}
\end{figure}

\section{3D Scene Reconstruction}\label{sec:3dreconstruction}
The ability to estimate the full 3D structure of the environment is often an essential requirement in an industrial scenario, since many object detection and localization techniques are based on 3D approaches, so efficient methods for reconstructing the scene are needed. As introduced in the previous section, most of the more promising depth sensors, both passive or active, are based on ``match and triangulate'' techniques: in this section we introduce a selection of effective 3D scene reconstruction algorithms that exploit this techniques. We will focus on 3D stereo reconstruction approaches, since they are by far the most popular and since they can be tested using our experimental sensor (\mynote{Mettere riferimento a sezione precedente ove inizi a introdurre questo sensore})

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/1_perception_and_sensing_in_robotics/binocular_disparity}
    \caption{\textbf{Image disparity and Depth}. The drawing describes the relationship of image displacement to depth with stereoscopic images, assuming flat co-planar images. Image taken from \url{https://en.wikipedia.org/wiki/Computer_stereo_vision}.} 
    \label{fig:binocular_disparity}
\end{figure}

\subsection{Stereo Matching}\label{subsec:stereomatching}
Stereo matching is used to correlate points from one digital image of a stereo pair with the corresponding points in the second image of the pair. However, finding the best algorithms and parameters, is usually difficult, since different aspects must be considered: accuracy, completeness, occlusions, computational efforts, etc. The approach of using stereo vision to acquire object's 3D geometric information is on the basis of visual \emph{disparity}. The concept of disparity in computer vision basically refers to the difference in coordinates of similar features within two stereo images. This concept is strictly related with what actually happens in our visual apparatus. Our binocular sight is also called parallax, given the horizontal displacement of our eyes. This disposition of the point of views generates the so called binocular disparity and as like as the definition we gave before, it is nothing more than the difference in image location of an object seen by the left and right eyes. From this image difference, it can be easily derived a relationship with the 3D world, namely we are able to reconstruct the depth of a given observed point starting from those displacements. The drawing in \figref{fig:binocular_disparity} shows an explanation of the relationship existing between image displacement, namely pixels' disparity, and the depth. In the drawing, light from the point $A$ is transmitted through the entry points of a pinhole cameras at $B$ and $D$, onto image screens at $E$ and $H$. The two camera lens are respectively at distance $BD = BC + CD$. That means that there are two couple of similar triangles, namely the triangle couples $(ACB, BFE)$ and $(ACD, DGH)$. So the displacement $d$ between the two points $E$ and $H$ on the two images is given by:

\begin{equation}
    \label{eq:disparity}
    \begin{aligned}
    d &= EF + GH \\
      &= BF(\dfrac{EF}{BF} + \dfrac{GH}{BF}) \\
      &= BF(\dfrac{EF}{BF} + \dfrac{GH}{DG}) \\ 
      &= BF(\dfrac{BC+CD}{AC}) \\
      &= BF(\dfrac{BD}{AC}) \\
      &= \dfrac{k}{z}
    \end{aligned}
\end{equation}

where $k = BD * BF$ and $z = AC$ is the distance from the camera plane to the object, namely the \emph{reconstructed depth} of the pixel. So assuming the cameras are level, and image planes are flat on the same plane (images must be rectified), the displacement in the horizontal axis between the same pixel in the two images is given by:

\begin{equation}
    \label{eq:disparity1}
    d = \dfrac{k}{z}
\end{equation}

where $k$ is the distance between the two cameras times the distance from the lens to the image. Moreover, the depth component in the two images are $z_{1}$ and $z_{2}$, given by:

\begin{equation}
    \label{eq:depth}
    \begin{aligned}
    z_1(x, y) = \min\{v : v = z_2(x, y - \dfrac{k}{z_2(x,y)})\} \\
    z_2(x, y) = \min\{v : v = z_1(x, y - \dfrac{k}{z_1(x,y)})\}
    \end{aligned}
\end{equation}

There are many techniques that exploit this brief and intuitive geometrical concept, in particular we will now address two popular techniques, namely \emph{Semi-Global Matching} from  \cite{hirschmuller2005SemiGlobal} and \emph{PatchMatch Stereo} from \cite{bleyer2011PatchMatchStereo}. Those techniques aims to find the same disparity ideally introduced before by exploiting image features and pixel color information. They have been also intensively used as benchmark tools for our RAW dataset and our experimental sensor setup, that is why we are now going to approach to them more in detail.


\subsubsection{Semi-Global Matching (SGM)}\label{subsec:semiglobalmatching}
The Semi-Global matching is, actually, one of the best matching strategies used both in photogrammetry and computer vision, offering good results with low runtime. It is commonly used in many real-world applications and implemented inside the firmware of several stereo based depth-cameras. The Semi-Global Matching method \cite{hirschmuller2005SemiGlobal} performs a pixel-wise matching allowing to shape efficiently object boundaries and fine details. 

The algorithm works with a pair of images, and their camera parameters, either intrinsic and extrinsic parameters must be known. That is because it assumes to know the epipolar constraint between the two images, the two cameras actually, so corresponding points are assumed to lie on the same horizontal image line, e.g. images have been rectified using such epipolar constraints. This algorithm realizes the minimization of a global smoothness constraint, combining matching costs along independent one-dimensional paths trough the image. The novel idea in SGM is that it actually computes the pixel matching cost through several paths in the image, not only on the horizontal epipolar line direction like it happened with previous implementations, e.g. scanline algorithm. All those paths and pixel-disparity pairs, aggregate in the computation of a cost function that SGM tries to minimize.

More in detail, the cost function $L^{\prime}_r(p, d)$ of the pixel $p$ and disparity $d$, along the path $r$ is defined as follow:

\begin{equation}\label{eq:sgm_cost_func}
    \begin{aligned}
    L^{\prime}_r(p, d) = &C(p, d) + 
    \min(L_r(p-r, d), \\
    &L_r(p-r, d-1)+P_1, \\
    &L_r(p-r, d+1)+P_2, \\
    &\min_iL_r(p-r,i)+P_2) - \min_kL_r(p-r,k)
    \end{aligned}
\end{equation}

In Eq. \ref{eq:sgm_cost_func} the first term $C(p, d)$ is the similarity cost (i.e. a value that penalizes, using appropriate metrics, solutions where different radiometric values are encountered in the neighbor area of the corresponding points), the second minimization term is for evaluating the regularity of the disparity field, in fact there are two penalization terms $P_1$ and $P_2$ that basically control small and large change in the disparity with respect to the previous point along the matching path $r$. In the end there is a regularization term that allows to reduce the final value subtracting the minimum path cost of the previous pixel from the amount, if no regularization term is used, the cost tends to gradually increase during cost aggregation along the path.

Minimizing such cost function for a 2D image space is a NP-complete problem. SGM minimizes the cost by means of Dynamic Programming, with the novel idea of computing the optimization combining several individual path, symmetrically from all directions through the image. Summing the path costs in all directions and searching the disparity with the minimal cost for each image pixel $p$, produces the final disparity map.

\subsubsection{PatchMatch Stereo}\label{subsubsec:patchmatchstereo}
To do ...

\section{Object Detection}\label{sec:objectdetection}
All the robotics application introduced in \secref{sec:roboticsinindustry} requires to detect and localize one or more known objects in the working area: we will introduce here some machine vision algorithms and techniques for object detection and localization. In particular it will be shown a comparison between standard state of the art techniques based on shape-based approaches, and deep learning networks oriented to object detection and recognition.

\subsection{The object detection and localization}
\mynote{Metti qui delle brevi ma chiare definizioni dei problemi}
\subsection{Template based approaches}
\mynote{Spiegare in generale il concetto del template matching}
\subsubsection{D\textsuperscript{2}{CO} Algorithm}\label{subsec:d2co}
\mynote{D2CO è solo un'istanza delle etecniche template based}
As already anticipated, standard approaches are based on object shape and appearance. This means that a CAD model of the object is needed, since the algorithm mainly works on edges extraction and some sort of template matching between the projected model and the edges extracted from the image.

An example of standard algorithm that is totally based on shape and edges is the one presented in \cite{imperoli2015d2co}. This is the very same approach used also in RoboCup@Work by the SPQR@Work team for performing the final registration step in the object detection pipeline. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/d2co_00}
    \caption{\textbf{D\textsuperscript{2}CO input data example}. The raster models used in D\textsuperscript{2}CO are computed from the 3D CAD models of the objects.} 
    \label{fig:d2co_00}
\end{figure}

The algorithm is called D\textsuperscript{2}{CO}, namely Direct Directional Chamfer Optimization. D\textsuperscript{2}CO refines the object position employing a non-linear optimization procedure, where the cost being minimized is extracted directly from a 3D image tensor composed by a sequence of distance maps. No ICP-like iterative re-association steps are required: the data association is implicitly optimized while inferring the object pose. This approach is able to handle textureless and partially occluded objects and does not require any off-line object learning step: just a 3D CAD model is needed (see Figure \ref{fig:d2co_00}).

The Directional Chamfer Distance (DCD) tensor encodes the minimum distance of an image point to an edge point in a joint direction/location space. Unlike the Chamfer distance, the DCD takes into account also the edges directions, see Figure \ref{fig:d2co_01} for further explanations.

The algorithm extract a set of object candidates by pre-computing the (projected) raster templates along with their image orientations for a large number of possible 3D locations; for each point, it looks up the DCD tensor, computing the template average distance. The templates are then sorted for increasing distances. D\textsuperscript{2}CO finally refines the object position employing a non-linear optimization procedure that minimizes a tensor-based cost function, look at Figure \ref{fig:d2co_02} for a registration example. Then, the Levenberg Marquardt algorithm with a Huber loss function is used in order to reduces the influence of outliers. During the optimization, both the (projected) points position and the (projected) points orientation are constantly updated.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/d2co_01}
    \caption{\textbf{Directional Chamfer Distance}. DCD tensor computation pipeline.} 
    \label{fig:d2co_01}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/d2co_02}
    \caption{\textbf{D\textsuperscript{2}{CO} Registration Example}. An example of object registration using the D\textsuperscript{2}{CO} algorithm.} 
    \label{fig:d2co_02}
\end{figure}

\subsection{Deep Learning Neural Networks for Object Detection}\label{subsec:dl_obj_detection}
Despite standard techniques for object detection like the one introduced in the last section, here we are going to explore some techniques that in the recent years have become the state of the art in most of the common computer vision tasks, such as object detection. Those approaches are based on \emph{CNNs (Convolutional Neural Networks)} and are commonly associated with the Deep Learning literature. In Machine Learning in fact, CNN is a class of deep, feed-forward artificial neural network. 

The idea of this kind of networks comes from biological processes: 

\begin{itemize}
	\item The connectivity pattern between neurons is inspired by the organization of the animal visual cortex; 	
	\item Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field; 
	\item The receptive fields of different neurons partially overlap such that they cover the entire visual field.
\end{itemize}

Following those intuitions, also in Computer Vision, more in detail in image processing, CNNs have been applied analyze and process visual data such as our brain does. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage and is something that comes directly from the input data used during the training phase of the network, that is why this kind of approaches are also called \emph{data driven}, because they mostly relies on huge amounts of data.

More in detail a CNN consists of an input and an output layer, as well as multiple hidden layers. Those hidden layers typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers. In particular:

\begin{itemize}
	\item \textbf{Convolutional Layer}: Convolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli. Each neuron here processes data only for its receptive field. Those kind of input partitioning allows CNNs to tolerate translation of the input image (e.g. translation, rotation, perspective distortion, etc.)
	\item \textbf{Pooling Layer}: Those kind of layers combine the outputs of neuron clusters at one layer into a single neuron in the next layer. For example, max pooling uses the maximum value from each of a cluster of neurons at the prior layer. Pooling layers are commonly used after convolutional ones in order to reduce the dimension of the data, also called data dimensionality reduction.
	\item \textbf{Fully Connected Layer}: Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP).
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/typical_cnn}
    \caption{\textbf{Typical CNN architecture}. Typical CNNs architectures are composed by single or many convolutional layers, each followed by some pooling layer in order to reduce the dimension of the data for the next convolutional layer. As our scope is to evaluate object detection and classification results, commonly used architectures uses fully connected layer at the end of the network for performing classification.} 
    \label{fig:typical_cnn}
\end{figure}

An example of a typical CNN architecture is given in Figure \ref{fig:typical_cnn}. One real architecture that we also tested with our RAW Dataset is called \emph{YOLO (You Only Look Once)} from \cite{Redmon2016YOLO}, and its secon improvement from \cite{Redmon2017YOLO2} which realized state of the art results on standard detection tasks like \emph{PASCAL VOC} and \emph{COCO}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/yolo_architecture}
    \caption{\textbf{YOLO Architecture}. This detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating $1 \times 1$ convolutional layers reduce the features space from preceding layers.} 
    \label{fig:yolo_architecture}
\end{figure}

\subsubsection{YOLO (You Only Look Once) Deep Neural Network Architecture}\label{subsubsec:yolo}
The revolutionary YOLO architecture is a very deep neural network composed by 24 convolutional layers followed by 2 fully connected layers. A detailed view of this architecture is given in Figure \ref{fig:yolo_architecture}.

This has been considered a very novel and interesting idea since it realizes detection as a regression problem, in particular it divides the image into an $S \times S$ grid and for each grid cell predicts $B$ bounding boxes, confidence for those boxes, and $C$ class probabilities. A very useful example of this concept is given in Figure \ref{fig:yolo_detail}, where it is explained how the process goes on. Basically, instead of directly predicting the bounding box of the object, they perform regression on the offsets between some pre-computed bounding boxes used as test and the real one. In this way, the process is much more easy and fast.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/1_perception_and_sensing_in_robotics/yolo_detail}
    \caption{\textbf{YOLO Image Analysis}. The image is sampled with a specific window, then each cell is then converted into class probability map and fused with the bounding boxes and their confidence in order to obtain the final decisions.} 
    \label{fig:yolo_detail}
\end{figure}

\section{State of the art Software Libraries in Industry}\label{sec:industrylibraries}
Machine Vision is one of the most active area in industrial settings. Over the past years, many software companies and Open Source communities have dedicated lot of effort in developing robust and effective techniques and algorithms in order to assist industrial realities, such as companies and start ups, in performing computer vision assisted tasks, e.g. random bin picking, Pick\&Place tasks and so on.

In the following subsection a list of tools and libraries will be introduced, focusing mainly on the MVTec's Halcon Libraries, which are the one that we used in the experiment phase of this work. 

\subsection{Halcon Libraries}\label{subsec:halconlibs}
Halcon\footnote{http://www.mvtec.com/products/halcon/}, from MVTec, is a set of commercial software developed and sold explicitly for industrial settings. Over the past 5 years it has become the state of the art in machine vision for industrial tasks. It serves all industries with an extensive library of more than 1600 operators for blob analysis, morphology, matching, measuring, identification, and 3D vision, to name just a few.

The full library can be accessed from common programming languages like C, C++, C\#, Visual Basic .NET, and Delphi. In particular, our tests have been developed using the C++ APIs. In the following chapters we will test this standard Machine Vision approaches over the RAW and T-Less datasets, and compare them with completely different approaches such as Deep Learning CNNs for object localization and recognition.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/1_perception_and_sensing_in_robotics/hdevelop_gui_example}
    \caption{\textbf{Hdevelop GUI example.} An example of using the Hdevelop software from the Halcon Libraries. In particular here we are performing an object detection and localization task.} 
    \label{fig:hdevelop_example}
\end{figure}

The Halcon Library has also an interactive and friendly GUI, provided in order to facilitate the interfacing with the low level software APIs. The aforementioned software tool is called HDevelop, and an example of its usage and graphical interface is depicted in Figure \ref{fig:hdevelop_example}.

As anticipated, this software library is under commercial license, and our distribution has been sold to La Sapienza University of Rome that can use it for research and other non-commercial purposes. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/1_perception_and_sensing_in_robotics/mil_gui_example}
    \caption{\textbf{MIL GUI example.} The graphical user interface of the Matrox Imaging Library.} 
    \label{fig:mil_example}
\end{figure}

\subsection{Matrox Imaging Library (MIL)}\label{subsec:mil}
Another important software library that needs to be mentioned is the Matrox Imaging Library\footnote{https://www.matrox.com/imaging/en/products/software/mil/} (MIL). MIL is a complete collection of software tools for developing machine vision in lots of different scenarios, it is not restricted to the industrial one such as for the previously mentioned Halcon Libraries, but it covers also medical images applications and many others.

MIL includes also a graphic user interface for fast developing and prototyping of solutions. An example of this GUI is depicted in Figure \ref{fig:mil_example}.

This library is not part of the tests and examples performed during this work.