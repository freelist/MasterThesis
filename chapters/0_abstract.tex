\begin{abstract}
Having rigorous test bed as benchmark for novel technologies, theories and methods is one of the key factor for improving more and more the research in every field. This concept is even more prominent in highly structured and rigorous settings such as the industrial one. We focused in particular on those scenarios and tried to propose rigorous benchmark procedures and tools for testing widely spread sensors, such as \emph{Microsoft Kinect 2} or \emph{Intel Realsense SR300} involving standard tools of common use in research in industrial settings, e.g. \emph{RoboCup@Work} objects. As like as common commercial sensors, over the years, industry has seen a very fast growth of custom and complex sensors, such as structured light sensors, active and passive stereo perceptive systems and many others. For those last mentioned sensor categories, to the knowledge of the author, only one publicly available dataset existed up to the 2017, for this reason with this work we also introduce a novel dataset, the \emph{RAW Dataset}, built with multiple sensors with different features, namely \emph{passive} and \emph{active stereo cameras} and \emph{structured light sensors}. The dataset provides both rgb and depth images from 15 different scenes of objects in multiple arrangement, there are scenes with no clutter at all and other completely randomly built. All the scenes are provided with the ground truth 6D position of the objects. The ground truth will be used for evaluating some state-of-the art techniques for object detection and localization, as like as 3D scene reconstruction algorithms. This investigation will focus on properties that are important for practical applications in robotic industry, such as comparing the full 3D rigid transformation of the objects, evaluating the goodness of objects' bounding box detection, and some other relevant and specific 3D reconstruction approaches. The sensor setup of this dataset also introduces some novelty, namely we tested and developed a new sensor for passive and active stereo vision, the \emph{FlexSight Sensor}, built in fulfillment, behalf and supported by the European Community's project ECHORD++.
\end{abstract}

\chapter*{Nomenclature}\label{ch:symbols}
\addcontentsline{toc}{chapter}{Nomenclature}

%\section*{Notation}
%\refstepcounter{notation}
%\label{sec:notation}
%
%To do ...
%\begin{tabbing}
%	\hspace*{3.0cm}		\= \kill
%	$^A\rot_{B}(\alpha, \beta, \gamma)$ \> Rotation from frame A to frame B expressed in A \\[0.75ex]
%	$^A\trans_{B}(t_x,t_y,t_z)$ \> Translation from frame A to frame B expressed in A  \\[0.75ex]
%	$\T{A}{B}(R,\trans)$ \> Transformation from frame A to B expressed in A  \\[0.75ex]
%	$\t2v{A}{B}(R,\trans)$ \> Transformation from frame A to B expressed in A (vectorized) \\[0.75ex]
%	${\lfloor\trans\rfloor}_\times$ \> Skew-symmetric matrix of a vector $\mathbf{t} \in \mathbb{R}^{3\times3}$
%\end{tabbing}

\section*{Acronyms and Abbreviations}
\label{sec:acronyms}

\begin{tabbing}
	\hspace*{3.5cm}		\= \kill
	API \> Application Programming Interface \\[1ex]
	GUI \> Graphical User Interface \\[1ex]
	CNN \> Convolutional Neural Network \\[1ex]
	MLP \> Multi-Layer Perceptron \\[1ex]
	OpenCV  \> Open source Computer Vision (library), \url{www.opencv.org} \\[1ex]
	Git  \> Git revision control, \url{www.git-scm.com} \\[1ex]
	RBP  \> Random Bin Picking \\[1ex]
	PPT  \> Pick and Place Task \\[1ex]
	IoU  \> Intersection over Union \\[1ex]
	AD  \> Average Distance \\[1ex]
	VSD  \> Visible Surface Distance \\[1ex]
	d.o.f.  \> Degrees of freedom \\[1ex]	
\end{tabbing}